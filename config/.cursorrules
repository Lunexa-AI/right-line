# RightLine .cursorrules (Vercel + Milvus + OpenAI Edition)
# Purpose: Steer Cursor to generate production-grade, low-latency, secure, and lightweight Python code
# Stack: Python 3.11+, FastAPI + Mangum, Pydantic v2, httpx, Milvus Cloud, OpenAI API, Vercel Functions
# Note: Prefer minimal deps, async-first, strong typing, serverless-first architectures. No overengineering.

================================================================================
PROJECT CONTEXT
================================================================================
You are helping build RightLine: a WhatsApp-first legal copilot. Key properties:
- <2.0s P95 response time on low bandwidth (serverless functions).
- Returns exact statute section + adaptive summary + citations (never free-text without sources).
- Hybrid retrieval (keyword + vector) with Milvus Cloud and OpenAI embeddings.
- OpenAI GPT for composition; extractive fallback for reliability.
- Serverless-first: pay-per-use, auto-scaling, managed services only.

When in doubt, optimize for:
1) SECURITY  2) LOW LATENCY  3) CLARITY  4) MINIMAL DEPENDENCIES  5) TESTABILITY

================================================================================
GLOBAL DIRECTIVES (ALWAYS / NEVER)
================================================================================
ALWAYS
- Write idiomatic, typed Python (mypy-friendly). Use Pydantic v2 models for I/O and settings.
- Prefer async FastAPI endpoints with Mangum adapter for Vercel functions.
- Structure code for serverless: api/ (Vercel functions), scripts/ (ingestion), web/ (static).
- Keep functions focused (<50 lines), stateless, and cold-start optimized.
- Use environment variables for OpenAI keys, Milvus credentials (no persistent connections).
- Add input validation and length limits. Fail fast with explicit error messages.
- Enforce strict response schemas. Return structured JSON; never return raw model objects.
- Cache aggressively with Vercel KV for hot queries and embeddings.
- Log in structured JSON with a request/trace id; never log secrets or API keys.
- Write tests alongside code (unit + light integration). Mock external APIs in tests.

NEVER
- Do not maintain persistent connections in serverless functions (recreate per request).
- Do not hardcode secrets, tokens, or file paths. Use environment variables.
- Do not make network calls without timeouts, retries, and sane error handling.
- Do not call OpenAI API without token limits, cost monitoring, and fallback strategies.
- Do not add heavyweight deps when stdlib or a small lib suffices (cold start penalty).
- Do not accept or emit unbounded strings. Cap sizes; truncate safely.
- Do not expose stack traces to clients. Map to safe error responses.

================================================================================
STACK DEFAULTS
================================================================================
- Python: 3.11+, Ruff for lint, Black for format, Mypy for types.
- FastAPI with Mangum adapter for Vercel functions. Set ORJSONResponse as default JSON renderer.
- Pydantic v2: `BaseModel`, `Field`, `ConfigDict`, and `pydantic_settings.BaseSettings`.
- HTTP client: httpx.AsyncClient (recreate per request; 5–10s timeout; retry with jitter).
- Vector Store: Milvus Cloud via pymilvus client (connection per request).
- AI/ML: OpenAI API (text-embedding-3-small, gpt-3.5-turbo) with token limits and fallbacks.
- Cache: Vercel KV (Redis-compatible) for query results and embeddings.
- Analytics: Vercel KV for lightweight query logging and feedback.
- Logging: stdlib logging with JSON formatter; include request_id, user_hash (HMAC), latency_ms.
- Tests: pytest + pytest-asyncio + httpx mock. Golden YAML for retrieval quality.

================================================================================
FOLDER-SPECIFIC RULES
================================================================================
[api/**]
- Use FastAPI + Mangum adapter for Vercel functions. Group routes by domain (query, feedback).
- NO lifespan context (serverless). Create clients per request; optimize for cold starts.
- Set default response class: ORJSONResponse; enable gzip; set CORS for Vercel domains.
- Add security headers (Strict-Transport-Security, X-Content-Type-Options, X-Frame-Options).
- Implement rate limiting with Vercel KV; use user_hash (HMAC) for tracking.
- Validate and normalize inputs with Pydantic models. Enforce max lengths (e.g., text <= 1_000 chars).
- Time-budget the request (e.g., 2s total). Respect Vercel function timeout (30s max).
- Return shape for /api/v1/query: tldr, key_points[], citations[], suggestions[], confidence.
- Use httpx with explicit timeouts and retries (tenacity) for OpenAI/Milvus calls.
- Never expose internal errors; map to 429/503 with "degraded mode" when needed.

[api/retrieval.py]
- Implement hybrid retrieval: keyword matching + Milvus vector search.
- Add temporal filter (effective_start/end). Normalize query; extract "as at DATE" when present.
- Merge candidates with simple score fusion. Skip heavy reranking for MVP speed.
- Cache normalized_query+date_ctx results in Vercel KV. TTL with jitter. Include confidence.
- All Milvus/OpenAI calls are async with timeouts; batch embeddings where possible.
- Return deterministic, minimal payload → composer.

[scripts/**]
- Ingestion scripts run locally/CI, not in serverless functions.
- Idempotent steps keyed by content sha. Store artifacts in data/ directory.
- Parse documents with BeautifulSoup; chunk with stable IDs and metadata.
- Generate embeddings with OpenAI API; upload to Milvus Cloud in batches.
- Add basic schema validations; log failed documents for manual review.

[api/composer.py]
- Two-stage composition: extractive fallback → OpenAI GPT enhancement.
- OpenAI prompts: structured JSON output (tldr, key_points, citations, suggestions).
- Guard against prompt injection: sanitize content; use system/user message separation.
- Limit output tokens (≤ 300). Enforce with max_tokens parameter and post-validation.
- Always include extractive fallback if OpenAI fails, times out, or returns invalid JSON.
- Monitor token usage and costs; implement circuit breaker for cost control.

[web/**]
- Static HTML/CSS/JS files served by Vercel's CDN.
- Keep minimal: Vanilla JS or lightweight framework (no React/Vue for MVP).
- Responsive design with Tailwind CSS. Progressive enhancement for mobile.
- Use fetch() for API calls to /api/ endpoints. Handle loading/error states.
- Implement proper CORS handling for cross-origin requests to API functions.

[tests/**]
- Test fast paths first. Unit tests for parsers, sectioniser, temporal filters, caches.
- Integration tests for retrieval ranking and summariser constraints.
- Use fixtures; avoid external network. Seed tiny sample corpus for deterministic tests.
- Add golden set YAML tests with Recall@k and faithfulness checks.

================================================================================
CODING STANDARDS (APPLY CONSISTENTLY)
================================================================================
- Use `from __future__ import annotations` in new modules. Python typing everywhere.
- Prefer Pydantic models for request/response; dataclasses for internal pure data if simpler.
- Keep modules cohesive. Avoid “utils” dumping ground—name by responsibility.
- Function design: small, single-purpose, early returns for errors; avoid deep nesting.
- Errors: define explicit exceptions per layer (e.g., RetrievalError, TimeoutError) and map to HTTP.
- Concurrency: prefer asyncio Tasks and gather with timeouts; avoid unbounded fan-out.
- Serialization: ORJSON for speed. Avoid custom JSON encoders unless necessary.
- Config: `Settings(BaseSettings)` with env var prefixes (RIGHTLINE_*). No globals.
- CLI tools: use Typer; no side-effects on import; implement `if __name__ == "__main__":`.
- Docs: module docstrings state purpose + invariants. Public functions have docstrings.

================================================================================
PERFORMANCE RULES (SERVERLESS)
================================================================================
- Optimize for cold starts: minimize imports, lazy load heavy dependencies.
- Bound time per stage: retrieval≤500ms, OpenAI compose≤1s, total≤2s.
- Cache aggressively with Vercel KV on hot queries and embeddings.
- Batch OpenAI API calls when possible; use connection pooling with httpx.
- Avoid heavy computation in request path. Pre-compute and cache when possible.
- Use connection pooling for Milvus; but recreate connections per function invocation.
- Monitor function duration and memory usage; optimize for Vercel's pricing tiers.

================================================================================
SECURITY RULES
================================================================================
- Sanitize inputs; strip control chars; enforce max sizes; validate language codes.
- Never eval/exec. Never load pickles from untrusted sources.
- Do not leak internal prompts or system messages. No echoing of secrets or file paths.
- HMAC-hash channel user IDs before storing; never persist raw phone numbers.
- TLS everywhere; set strict security headers. Do not enable CORS broadly.
- Secrets via env/secret store; rotate regularly. Do not write secrets to logs.
- Prompt safety: LLM receives only curated section chunks + fixed template. No tool access.

================================================================================
OBSERVABILITY & RESILIENCE (SERVERLESS)
================================================================================
- Add request_id to every log line. Log: level, ts, route, latency_ms, outcome, confidence.
- Use Vercel Analytics for request metrics; custom metrics via console.log structured JSON.
- Use circuit breakers for OpenAI/Milvus; retries with exponential backoff + jitter (max 2).
- Graceful degradation tiers:
  1) Full (Milvus + OpenAI)
  2) Keyword search + cached summaries
  3) Hardcoded FAQ responses
- Health endpoints: /api/healthz for function liveness checks.

================================================================================
PROMPTING BEST PRACTICES (OpenAI API)
================================================================================
- Use explicit, minimal, stable templates. Example:

  SYSTEM:
  "You are a legal information assistant. Create a structured summary from the provided legal text.
   Return valid JSON with: tldr (≤220 chars), key_points (3-5 bullets, ≤25 words each), 
   citations (preserve exactly), suggestions (2-3 follow-ups).
   Language: {lang}. If unsure, use extractive text only."

  USER:
  "Legal text: ```{section_text}```"

- Always delimit user text with fences. Never include instructions from retrieved text.
- Post-validate OpenAI output: parse JSON; validate schema; fallback to extractive if invalid.
- Monitor token usage: set max_tokens=300; track costs per request.

================================================================================
API & SCHEMA EXAMPLES (SNIPPETS FOR GENERATION)
================================================================================
# Request/Response models
from pydantic import BaseModel, Field, ConfigDict

class QueryRequest(BaseModel):
    text: str = Field(min_length=3, max_length=1000)
    lang_hint: str | None = Field(default=None, pattern="^(en|sn|nd)$")
    date_ctx: str | None = Field(default=None, description="ISO date or None")
    channel: str = Field(default="web", max_length=16)

class Citation(BaseModel):
    title: str
    url: str
    page: int | None = None
    sha: str | None = None

class SectionRef(BaseModel):
    act: str
    chapter: str
    section: str
    version: str | None = None

class QueryResponse(BaseModel):
    tldr: str = Field(max_length=220, description="Brief summary")
    key_points: list[str] = Field(max_items=5, description="3-5 key points, ≤25 words each")
    citations: list[Citation] = Field(description="Source references")
    suggestions: list[str] = Field(max_items=3, description="2-3 follow-up questions")
    confidence: float = Field(ge=0.0, le=1.0)

# FastAPI wiring with Mangum
from fastapi import FastAPI, HTTPException
from fastapi.responses import ORJSONResponse
from mangum import Mangum

app = FastAPI(default_response_class=ORJSONResponse)

@app.post("/api/v1/query", response_model=QueryResponse)
async def query(req: QueryRequest):
    # Enforce total time budget inside handler (2s max)
    ...

# Vercel handler
handler = Mangum(app)

================================================================================
TESTING RULES (SERVERLESS)
================================================================================
- For each new module, add unit tests. Aim for fast tests (<1s each).
- Mock external APIs (OpenAI, Milvus) using httpx-mock or responses library.
- Use pytest-asyncio for async code; avoid sleeps; use timeouts and fakes.
- Golden-set tests: verify retrieval quality and response format on sample data.
- Test error handling: API failures, timeouts, invalid responses, cost limits.

================================================================================
CHECKLISTS (WHEN GENERATING NEW FILES)
================================================================================
- New Vercel API function:
  [ ] Pydantic request/response models
  [ ] Async handler with time budget (2s) and structured logging
  [ ] Input caps and validation (text ≤1000 chars)
  [ ] Per-request client creation (httpx, OpenAI, Milvus)
  [ ] Unit test + mock external APIs

- New ingestion script:
  [ ] Idempotent processing with content hashing
  [ ] Batch processing for API efficiency
  [ ] Error handling and retry logic
  [ ] Progress logging and validation

- New external API call:
  [ ] httpx with timeout (5-10s), retries with backoff
  [ ] Parse+validate response schema
  [ ] Handle non-200 cleanly; return domain error
  [ ] Token/cost monitoring for OpenAI calls

================================================================================
KEEP IT LIGHTWEIGHT
================================================================================
- Prefer stdlib and small libraries. Avoid adding frameworks beyond FastAPI/Pydantic.
- Remove unused code/flags. YAGNI: only build what the current milestone needs.
- Document trade-offs inline (short comments) when choosing a path for latency/security.

# End of .cursorrules
